{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "673"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from project_config import PREPROCESSED_2012_DATA_FILE, PREPROCESSED_2022_DATA_FILE\n",
    "from utils.file_encoding_detector import detect_encoding\n",
    "from utils.read_csv_file import read_csv_file\n",
    "\n",
    "file_2012 = PREPROCESSED_2012_DATA_FILE\n",
    "file_2022 = PREPROCESSED_2022_DATA_FILE\n",
    "\n",
    "df_1 = read_csv_file(file_2012)\n",
    "df_2 = read_csv_file(file_2022)\n",
    "\n",
    "df_2.group.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_config import YEARBOOK_2012_DATA_DIR\n",
    "\n",
    "\n",
    "source_data_dir = YEARBOOK_2012_DATA_DIR\n",
    "\n",
    "files_to_check_against = list(source_data_dir.glob(\"*.xls*\"))\n",
    "\n",
    "print(files_to_check_against)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17403 98742\n"
     ]
    }
   ],
   "source": [
    "from project_config import TRAINING_DATA_FILE, TRAINING_INFERENCE_DATA_FILE\n",
    "from utils.read_csv_file import read_csv_file\n",
    "\n",
    "df1 = read_csv_file(TRAINING_DATA_FILE)\n",
    "df2 = read_csv_file(TRAINING_INFERENCE_DATA_FILE)\n",
    "\n",
    "print(len(df1), len(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load pre-trained Word2Vec model (Google News dataset)\n",
    "word_vectors = KeyedVectors.load_word2vec_format(\n",
    "    \"GoogleNews-vectors-negative300.bin\", binary=True\n",
    ")\n",
    "\n",
    "# Words to visualize\n",
    "words = [\"king\", \"queen\", \"man\", \"woman\"]\n",
    "vectors = [word_vectors[word] for word in words]\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(vectors)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i, word in enumerate(words):\n",
    "    plt.scatter(reduced_vectors[i, 0], reduced_vectors[i, 1], label=word)\n",
    "    plt.text(reduced_vectors[i, 0] + 0.02, reduced_vectors[i, 1], word, fontsize=12)\n",
    "plt.title(\"Word Embedding Visualization (King, Queen, Man, Woman)\")\n",
    "plt.xlabel(\"PCA Dimension 1\")\n",
    "plt.ylabel(\"PCA Dimension 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>yearbook_source</th>\n",
       "      <th>section</th>\n",
       "      <th>group</th>\n",
       "      <th>row_id</th>\n",
       "      <th>is_title</th>\n",
       "      <th>is_empty</th>\n",
       "      <th>original_index</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-1  Divisions of Administrative Areas in Chin...</td>\n",
       "      <td>2012</td>\n",
       "      <td>General Survey</td>\n",
       "      <td>B0101e</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPT...</td>\n",
       "      <td>2012</td>\n",
       "      <td>General Survey</td>\n",
       "      <td>B0101e</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(unit), EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMP...</td>\n",
       "      <td>2012</td>\n",
       "      <td>General Survey</td>\n",
       "      <td>B0101e</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPT...</td>\n",
       "      <td>2012</td>\n",
       "      <td>General Survey</td>\n",
       "      <td>B0101e</td>\n",
       "      <td>4</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Provinces  Autonomous , Number of, Cities, Num...</td>\n",
       "      <td>2012</td>\n",
       "      <td>General Survey</td>\n",
       "      <td>B0101e</td>\n",
       "      <td>5</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>header</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17398</th>\n",
       "      <td>United Kingdom, 0.66, 0.65, 0.78, 0.75, 0.78,...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Appendix II. A Comparison of Indicators of Eco...</td>\n",
       "      <td>E29-15</td>\n",
       "      <td>47</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>17398</td>\n",
       "      <td>table_data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17399</th>\n",
       "      <td>Australia, 1.73, 1.09, 1.31, 1.34, 1.44, 1.45...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Appendix II. A Comparison of Indicators of Eco...</td>\n",
       "      <td>E29-15</td>\n",
       "      <td>48</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>17399</td>\n",
       "      <td>table_data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17400</th>\n",
       "      <td>New Zealand, 2.2, 1.39, 1.41, 1.45, 1.52, 1.5...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Appendix II. A Comparison of Indicators of Eco...</td>\n",
       "      <td>E29-15</td>\n",
       "      <td>49</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>17400</td>\n",
       "      <td>table_data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17401</th>\n",
       "      <td>EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPT...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Appendix II. A Comparison of Indicators of Eco...</td>\n",
       "      <td>E29-15</td>\n",
       "      <td>50</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>17401</td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17402</th>\n",
       "      <td>EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPT...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Appendix II. A Comparison of Indicators of Eco...</td>\n",
       "      <td>E29-15</td>\n",
       "      <td>51</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>17402</td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17403 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  yearbook_source  \\\n",
       "0      1-1  Divisions of Administrative Areas in Chin...             2012   \n",
       "1      EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPT...             2012   \n",
       "2      (unit), EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMP...             2012   \n",
       "3      EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPT...             2012   \n",
       "4      Provinces  Autonomous , Number of, Cities, Num...             2012   \n",
       "...                                                  ...              ...   \n",
       "17398   United Kingdom, 0.66, 0.65, 0.78, 0.75, 0.78,...             2022   \n",
       "17399   Australia, 1.73, 1.09, 1.31, 1.34, 1.44, 1.45...             2022   \n",
       "17400   New Zealand, 2.2, 1.39, 1.41, 1.45, 1.52, 1.5...             2022   \n",
       "17401  EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPT...             2022   \n",
       "17402  EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPT...             2022   \n",
       "\n",
       "                                                 section   group  row_id  \\\n",
       "0                                         General Survey  B0101e       1   \n",
       "1                                         General Survey  B0101e       2   \n",
       "2                                         General Survey  B0101e       3   \n",
       "3                                         General Survey  B0101e       4   \n",
       "4                                         General Survey  B0101e       5   \n",
       "...                                                  ...     ...     ...   \n",
       "17398  Appendix II. A Comparison of Indicators of Eco...  E29-15      47   \n",
       "17399  Appendix II. A Comparison of Indicators of Eco...  E29-15      48   \n",
       "17400  Appendix II. A Comparison of Indicators of Eco...  E29-15      49   \n",
       "17401  Appendix II. A Comparison of Indicators of Eco...  E29-15      50   \n",
       "17402  Appendix II. A Comparison of Indicators of Eco...  E29-15      51   \n",
       "\n",
       "      is_title is_empty  original_index       label  \n",
       "0          yes       no               0       title  \n",
       "1           no      yes               1       empty  \n",
       "2           no       no               2    metadata  \n",
       "3           no      yes               3       empty  \n",
       "4           no       no               4      header  \n",
       "...        ...      ...             ...         ...  \n",
       "17398       no       no           17398  table_data  \n",
       "17399       no       no           17399  table_data  \n",
       "17400       no       no           17400  table_data  \n",
       "17401       no      yes           17401       empty  \n",
       "17402       no      yes           17402       empty  \n",
       "\n",
       "[17403 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from project_config import TRAINING_DATA_FILE\n",
    "from utils.read_csv_file import read_csv_file\n",
    "\n",
    "df = read_csv_file(TRAINING_DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Final Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference file blank text cells: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "root - INFO - Detected encoding: ISO-8859-1 with confidence 0.73\n",
      "root - INFO - File read successfully from C:\\github\\table_lense\\pipeline_data\\training_inference\\training\\training_data_v1.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_training file blank text cells: 0\n",
      "cleaned_training file blank text cells: 93\n",
      "combined file blank text cells: 93\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils.read_csv_file import read_csv_file\n",
    "\n",
    "from project_config import (\n",
    "    INFERENCE_INPUT_DATA_FILE,\n",
    "    RAW_INFERENCE_OUTPUT_DATA_FILE,\n",
    "    CLEANED_INFERENCE_OUTPUT_DATA_FILE,\n",
    "    TRAINING_DATA_FILE,\n",
    "    CLEANED_TRAINING_OUTPUT_DATA_FILE,\n",
    "    COMBINED_CLEANED_OUTPUT_DATA_FILE,\n",
    ")\n",
    "\n",
    "df = pd.read_csv(CLEANED_INFERENCE_OUTPUT_DATA_FILE)\n",
    "blank_cells = df[\"text\"].isnull().sum()\n",
    "print(f\"inference file blank text cells: {blank_cells}\")\n",
    "\n",
    "df = read_csv_file(TRAINING_DATA_FILE)\n",
    "blank_cells = df[\"text\"].isnull().sum()\n",
    "print(f\"original_training file blank text cells: {blank_cells}\")\n",
    "\n",
    "df = pd.read_csv(CLEANED_TRAINING_OUTPUT_DATA_FILE)\n",
    "blank_cells = df[\"text\"].isnull().sum()\n",
    "print(f\"cleaned_training file blank text cells: {blank_cells}\")\n",
    "\n",
    "df = pd.read_csv(COMBINED_CLEANED_OUTPUT_DATA_FILE)\n",
    "blank_cells = df[\"text\"].isnull().sum()\n",
    "print(f\"combined file blank text cells: {blank_cells}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "root - INFO - Cleaning training data from: C:\\github\\table_lense\\pipeline_data\\training_inference\\training\\training_data_v1.csv\n",
      "data_processing.post_process_inference_data - INFO - Start processing file (C:\\github\\table_lense\\pipeline_data\\training_inference\\training\\training_data_v1.csv)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Retrieved username: xzhan\n",
      "[DEBUG] Log file path: C:\\github\\table_lense\\logs\\xzhan_20250322_172204.log\n",
      "[DEBUG] Logging successfully configured with a single file handler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data_processing.post_process_inference_data - INFO - Detected encoding: ISO-8859-1 with confidence 0.73\n",
      "data_processing.post_process_inference_data - INFO - File read successfully from C:\\github\\table_lense\\pipeline_data\\training_inference\\training\\training_data_v1.csv\n",
      "data_processing.post_process_inference_data - INFO - DataFrame loaded:\n",
      "                                                text  yearbook_source  \\\n",
      "0  1-1  Divisions of Administrative Areas in Chin...             2012   \n",
      "1  EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPT...             2012   \n",
      "2  (unit), EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMP...             2012   \n",
      "3  EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPT...             2012   \n",
      "4  Provinces  Autonomous , Number of, Cities, Num...             2012   \n",
      "\n",
      "          section   group  row_id is_title is_empty  original_index     label  \n",
      "0  General Survey  B0101e       1      yes       no               0     title  \n",
      "1  General Survey  B0101e       2       no      yes               1     empty  \n",
      "2  General Survey  B0101e       3       no       no               2  metadata  \n",
      "3  General Survey  B0101e       4       no      yes               3     empty  \n",
      "4  General Survey  B0101e       5       no       no               4    header  \n",
      "data_processing.post_process_inference_data - INFO - Predicted label column not found; skipping conversion.\n",
      "data_processing.post_process_inference_data - INFO - Renamed column 'label' to 'actual_label'.\n",
      "data_processing.post_process_inference_data - INFO - New column 'label' is created.\n",
      "data_processing.post_process_inference_data - INFO - Merged actual labels; final 'label' initialized from 'actual_label'.\n",
      "data_processing.post_process_inference_data - INFO - New column 'label_type' is created.\n",
      "data_processing.post_process_inference_data - INFO - Starting set_label_type function.\n",
      "data_processing.post_process_inference_data - INFO - Set 'label_type' to 'predicted' for unlabeled rows, else 'actual'.\n",
      "data_processing.post_process_inference_data - INFO - Completed set_label_type function.\n",
      "data_processing.post_process_inference_data - INFO - Starting merge_labels_based_on_type function.\n",
      "data_processing.post_process_inference_data - WARNING - Predicted label column 'predicted_label' not found; using actual labels for 'label'.\n",
      "data_processing.post_process_inference_data - INFO - Completed merge_labels_based_on_type function.\n",
      "data_processing.post_process_inference_data - INFO - DataFrame sorted.\n",
      "c:\\github\\table_lense\\src\\data_processing\\post_process_inference_data.py:377: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(clean_text)\n",
      "data_processing.post_process_inference_data - INFO - Text cleaned.\n",
      "data_processing.post_process_inference_data - INFO - Starting label_empty_rows function.\n",
      "data_processing.post_process_inference_data - INFO - Applied empty row labeling based on text content in 'text' column.\n",
      "data_processing.post_process_inference_data - INFO - Forced 'label' to 'empty' for rows where 'is_empty' equals 'yes'.\n",
      "data_processing.post_process_inference_data - INFO - Completed label_empty_rows function.\n",
      "data_processing.post_process_inference_data - INFO - Empty rows updated.\n",
      "data_processing.post_process_inference_data - INFO - Starting label_title function.\n",
      "data_processing.post_process_inference_data - INFO - Set 'label' to 'title' for rows where 'is_title' is 'yes'.\n",
      "data_processing.post_process_inference_data - INFO - Completed label_title function.\n",
      "data_processing.post_process_inference_data - INFO - Title rows updated.\n",
      "data_processing.post_process_inference_data - INFO - Metadata checked & reclassified.\n",
      "data_processing.post_process_inference_data - INFO - Reclassification of empty, metadata, and header rows completed.\n",
      "data_processing.post_process_inference_data - INFO - Cleaned data saved to C:\\github\\table_lense\\pipeline_data\\training_inference\\training\\cleaned_training_data.csv\n",
      "root - INFO - Cleaned training data saved to: C:\\github\\table_lense\\pipeline_data\\training_inference\\training\\cleaned_training_data.csv\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from data_processing.post_process_inference_data import (\n",
    "    process_inference_training_results,\n",
    ")\n",
    "from project_config import TRAINING_DATA_FILE, CLEANED_TRAINING_OUTPUT_DATA_FILE\n",
    "\n",
    "# Optional: Setup logging if you want to see output\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "\n",
    "def clean_training_data():\n",
    "    \"\"\"\n",
    "    Re-clean the training input data using the same cleaning pipeline\n",
    "    used in the inference pipeline.\n",
    "    \"\"\"\n",
    "    input_file = Path(TRAINING_DATA_FILE)\n",
    "    output_file = Path(CLEANED_TRAINING_OUTPUT_DATA_FILE)\n",
    "\n",
    "    logging.info(f\"Cleaning training data from: {input_file}\")\n",
    "    process_inference_training_results(\n",
    "        input_file_path=input_file, output_file_path=output_file\n",
    "    )\n",
    "    logging.info(f\"Cleaned training data saved to: {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check training file, inference in, inference embeddings, training + inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total individual rows in training_embedding pickle file: 69634\n",
      "inference output data: 69634\n",
      "training and inference input data: 98742\n",
      "training input data: 17403\n",
      "\n",
      "total: 87037\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from project_config import (\n",
    "    INFERENCE_EMBEDDINGS_PKL_FILE,\n",
    "    TRAINING_INFERENCE_DATA_FILE,\n",
    "    TRAINING_DATA_FILE,\n",
    "    RAW_INFERENCE_OUTPUT_DATA_FILE,\n",
    ")\n",
    "import pandas as pd\n",
    "from utils.read_csv_file import read_csv_file\n",
    "\n",
    "partial_cache_path = Path(INFERENCE_EMBEDDINGS_PKL_FILE)\n",
    "\n",
    "with partial_cache_path.open(\"rb\") as f:\n",
    "    cache_data = pickle.load(f)\n",
    "\n",
    "results = cache_data.get(\"results\", [])\n",
    "total_rows = sum(batch[0].shape[0] for batch in results if hasattr(batch[0], \"shape\"))\n",
    "print(\"Total individual rows in training_embedding pickle file:\", total_rows)\n",
    "\n",
    "training_inference_data = read_csv_file(TRAINING_INFERENCE_DATA_FILE)\n",
    "training_data = read_csv_file(TRAINING_DATA_FILE)\n",
    "inference_output_data = read_csv_file(RAW_INFERENCE_OUTPUT_DATA_FILE)\n",
    "\n",
    "print(f\"inference output data: {len(inference_output_data)}\")\n",
    "print(f\"training and inference input data: {len(training_inference_data)}\")\n",
    "print(f\"training input data: {len(training_data)}\")\n",
    "\n",
    "print()\n",
    "total = len(training_data) + len(inference_output_data)\n",
    "print(f\"total: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicates in different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from project_config import (\n",
    "    INFERENCE_EMBEDDINGS_PKL_FILE,\n",
    "    TRAINING_INFERENCE_DATA_FILE,\n",
    "    TRAINING_DATA_FILE,\n",
    "    RAW_INFERENCE_OUTPUT_DATA_FILE,\n",
    ")\n",
    "import pandas as pd\n",
    "from utils.read_csv_file import read_csv_file\n",
    "\n",
    "train_inf_df = read_csv_file(TRAINING_INFERENCE_DATA_FILE)\n",
    "\n",
    "train_inf_df.original_index.nunique() == len(train_inf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check inference files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'original_index' column in Input Inference is unique.\n",
      "'original_index' column in Raw Output Inference is unique.\n",
      "'original_index' column in Cleaned Output Inference is unique.\n",
      "Input Inference: 69634\n",
      "Raw Output Inference: 69634\n",
      "Cleaned Output Inference: 69634\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from project_config import (\n",
    "    CLEANED_INFERENCE_OUTPUT_DATA_FILE,\n",
    "    INFERENCE_INPUT_DATA_FILE,\n",
    "    RAW_INFERENCE_OUTPUT_DATA_FILE,\n",
    ")\n",
    "from utils.read_csv_file import read_csv_file\n",
    "\n",
    "\n",
    "def load_and_validate_data(file_paths):\n",
    "    \"\"\"\n",
    "    Load data from CSV files and validate the 'original_indices' column.\n",
    "\n",
    "    Args:\n",
    "        file_paths (dict): A dictionary containing file paths with descriptive keys.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing loaded DataFrames.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for key, file_path in file_paths.items():\n",
    "        df = read_csv_file(file_path)\n",
    "        data[key] = df\n",
    "\n",
    "        # Check if 'original_index' column exists\n",
    "        if \"original_index\" in df.columns:\n",
    "            # Check if 'original_index' column is unique\n",
    "            if df[\"original_index\"].is_unique:\n",
    "                print(f\"'original_index' column in {key} is unique.\")\n",
    "            else:\n",
    "                print(f\"Warning: 'original_index' column in {key} is not unique.\")\n",
    "        else:\n",
    "            print(f\"Warning: 'original_index' column not found in {key}.\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def print_data_statistics(data):\n",
    "    \"\"\"\n",
    "    Print statistics about the loaded data.\n",
    "\n",
    "    Args:\n",
    "        data (dict): A dictionary containing loaded DataFrames.\n",
    "    \"\"\"\n",
    "    for key, df in data.items():\n",
    "        print(f\"{key}: {len(df)}\")\n",
    "\n",
    "\n",
    "# Define file paths\n",
    "file_paths = {\n",
    "    \"Input Inference\": INFERENCE_INPUT_DATA_FILE,\n",
    "    \"Raw Output Inference\": RAW_INFERENCE_OUTPUT_DATA_FILE,\n",
    "    \"Cleaned Output Inference\": CLEANED_INFERENCE_OUTPUT_DATA_FILE,\n",
    "}\n",
    "\n",
    "# Load and validate data\n",
    "data = load_and_validate_data(file_paths)\n",
    "\n",
    "# Print data statistics\n",
    "print_data_statistics(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'original_index' column in Cleaned Output Inference is unique.\n",
      "'original_index' column in Cleaned Training is unique.\n",
      "Cleaned Output Inference: 69634\n",
      "Cleaned Training: 17403\n",
      "Overlap between 'Cleaned Output Inference' and 'Cleaned Training': 15497\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from project_config import (\n",
    "    CLEANED_INFERENCE_OUTPUT_DATA_FILE,\n",
    "    INFERENCE_INPUT_DATA_FILE,\n",
    "    RAW_INFERENCE_OUTPUT_DATA_FILE,\n",
    "    CLEANED_TRAINING_OUTPUT_DATA_FILE,\n",
    ")\n",
    "from utils.read_csv_file import read_csv_file\n",
    "\n",
    "\n",
    "def load_and_validate_data(file_paths):\n",
    "    \"\"\"\n",
    "    Load data from CSV files and validate the 'original_indices' column.\n",
    "\n",
    "    Args:\n",
    "        file_paths (dict): A dictionary containing file paths with descriptive keys.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing loaded DataFrames.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for key, file_path in file_paths.items():\n",
    "        df = read_csv_file(file_path)\n",
    "        data[key] = df\n",
    "\n",
    "        # Check if 'original_index' column exists\n",
    "        if \"original_index\" in df.columns:\n",
    "            # Check if 'original_index' column is unique\n",
    "            if df[\"original_index\"].is_unique:\n",
    "                print(f\"'original_index' column in {key} is unique.\")\n",
    "            else:\n",
    "                print(f\"Warning: 'original_index' column in {key} is not unique.\")\n",
    "        else:\n",
    "            print(f\"Warning: 'original_index' column not found in {key}.\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def print_data_statistics(data):\n",
    "    \"\"\"\n",
    "    Print statistics about the loaded data.\n",
    "\n",
    "    Args:\n",
    "        data (dict): A dictionary containing loaded DataFrames.\n",
    "    \"\"\"\n",
    "    for key, df in data.items():\n",
    "        print(f\"{key}: {len(df)}\")\n",
    "\n",
    "\n",
    "def compare_original_index(data, file1, file2):\n",
    "    \"\"\"\n",
    "    Compare the 'original_index' columns between two files.\n",
    "\n",
    "    Args:\n",
    "        data (dict): A dictionary containing loaded DataFrames.\n",
    "        file1 (str): The key of the first file in the data dictionary.\n",
    "        file2 (str): The key of the second file in the data dictionary.\n",
    "    \"\"\"\n",
    "    # Get the 'original_index' columns from the two files\n",
    "    original_index1 = set(data[file1][\"original_index\"])\n",
    "    original_index2 = set(data[file2][\"original_index\"])\n",
    "\n",
    "    # Find the intersection of the two sets\n",
    "    overlap = original_index1.intersection(original_index2)\n",
    "\n",
    "    # Print the result\n",
    "    if overlap:\n",
    "        print(f\"Overlap between '{file1}' and '{file2}': {len(overlap)}\")\n",
    "    else:\n",
    "        print(f\"No overlap between '{file1}' and '{file2}'\")\n",
    "\n",
    "\n",
    "# Define file paths\n",
    "file_paths = {\n",
    "    # \"Input Inference\": INFERENCE_INPUT_DATA_FILE,\n",
    "    # \"Raw Output Inference\": RAW_INFERENCE_OUTPUT_DATA_FILE,\n",
    "    \"Cleaned Output Inference\": CLEANED_INFERENCE_OUTPUT_DATA_FILE,\n",
    "    \"Cleaned Training\": CLEANED_TRAINING_OUTPUT_DATA_FILE,\n",
    "}\n",
    "\n",
    "# Load and validate data\n",
    "data = load_and_validate_data(file_paths)\n",
    "\n",
    "# Print data statistics\n",
    "print_data_statistics(data)\n",
    "\n",
    "# Compare the 'original_index' columns between two files\n",
    "compare_original_index(data, \"Cleaned Output Inference\", \"Cleaned Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'original_index' column in Input Inference is unique.\n",
      "'original_index' column in Training is unique.\n",
      "Overlap between 'Input Inference' and 'Training': 15497\n"
     ]
    }
   ],
   "source": [
    "from project_config import TRAINING_DATA_FILE, INFERENCE_INPUT_DATA_FILE\n",
    "\n",
    "\n",
    "def has_unique_values(series):\n",
    "    return series.is_unique\n",
    "\n",
    "\n",
    "# Define file paths\n",
    "file_paths = {\n",
    "    \"Input Inference\": INFERENCE_INPUT_DATA_FILE,\n",
    "    \"Training\": TRAINING_DATA_FILE,\n",
    "}\n",
    "# Load and validate data\n",
    "data = load_and_validate_data(file_paths)\n",
    "compare_original_index(data, \"Input Inference\", \"Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data groups: {'E02-15', 'F0514e', 'E05-01', 'E02-07', 'E11-05', 'E08-02', 'H0806e', 'E08-03', 'E19-11', 'E29-09', 'E02-11', 'Q1801e', 'E27-30', 'E26-01', 'E02-09', 'E0412e', 'E26-03', 'E17-01', 'E25-01', 'E06-13', 'Z2607e', 'E10-02', 'E17-03', 'E07-08', 'E11-18', 'C0212e', 'D0302e', 'E26-24', 'E10-04', 'E11-15', 'E26-29', 'R0602e', 'E02-05', 'E01-06', 'E11-17', 'E28-19', 'T1904e', 'E06-15', 'E10-10', 'E16-40', 'E28-26', 'U2003e', 'Z2611e', 'E26-27', 'E18-14', 'Z2614e', 'E11-09', 'E03-01', 'E09-07', 'E13-04', 'E08-06', 'E24-02', 'E13-06', 'E01-02', 'T1903e', 'E12-08', 'E21-01', 'Z2612e', 'E10-09', 'E03-02', 'E09-02', 'E04-08', 'E18-16', 'E28-02', 'E07-04', 'E28-01', 'X2503e', 'H0807e', 'E04-06', 'D0305e', 'V2204e', 'E04-01', 'E28-28', 'Q1701e', 'E06-09', 'E19-04', 'E14-04', 'E19-12', 'E27-01', 'E24-04', 'E27-02', 'D0304e', 'E02-13', 'W2451e', 'T1918e', 'E11-02', 'C0215e', 'E20-20', 'E29-02', 'E06-11', 'E08-04', 'E27-29', 'E0401c', 'E09-01', 'E07-05', 'E18-03', 'V2103e', 'Z2604e', 'E01-07', 'E09-06', 'E18-15', 'X2501e', 'E28-20', 'E13-05', 'E29-01', 'E20-01', 'E07-07', 'E12-04', 'E07-02', 'E28-03', 'D0301e', 'E08-01', 'E28-18', 'E29-05', 'E02-08', 'W2403e', 'E21-02', 'E01-01', 'E12-06', 'F0519e', 'E28-17', 'E27-03', 'E11-08', 'E15-11', 'E29-03', 'E25-04', 'E02-01', 'E04-09', 'Z2506e', 'E19-02', 'Z2609e', 'E29-15', 'E06-32', 'Z2608e', 'N1401e', 'E20-21', 'V2201e', 'E07-06', 'E11-01', 'E02-14', 'T1905e', 'E11-10', 'E29-12', 'E18-17', 'R0622e', 'V2202e', 'E01-08', 'O1502e', 'E15-05', 'E22-02', 'R0601e', 'E26-25', 'E16-01', 'C0213e', 'E15-03', 'E23-02', 'E28-21', 'X2502e', 'E11-06', 'O1503e', 'E28-24', 'E28-22', 'U2004e', 'E06-01', 'E11-11', 'E29-13', 'V2101e', 'E25-02', 'E14-02', 'E29-08', 'R0621e', 'T1906e', 'E28-27', 'Z2605e', 'E08-07', 'E06-36', 'E06-05', 'E20-03', 'E19-17', 'P1602e', 'E06-07', 'E16-39', 'V2104e', 'Z2505e', 'E06-03', 'W2401e', 'E22-01', 'E20-02', 'E04-04', 'E09-04', 'Z2503e', 'E02-02', 'E15-13', 'E03-06', 'E05-02', 'Z2606e', 'E12-05', 'E29-14', 'E29-04', 'E10-03', 'E23-01', 'E05-04', 'E18-02', 'E26-02', 'E04-07', 'E11-07', 'E25-13', 'E25-03', 'E29-10', 'E08-05', 'E06-35', 'E08-36', 'E18-04', 'C0208e', 'E16-02', 'E10-12', 'E10-08', 'U2073e', 'E11-16', 'E26-04', 'E29-06', 'E26-26', 'E15-12', 'O1541e', 'O1501e', 'E17-02', 'E13-03', 'V2205e', 'E02-03', 'C0209e', 'E25-14', 'E14-06', 'E22-03', 'E05-06', 'E04-02', 'E05-05', 'E06-10', 'E09-10', 'E28-25', 'E18-01', 'E02-04', 'E15-02', 'E21-04', 'E25-15', 'P1637e', 'E01-03', 'U2076e', 'E13-01', 'E08-09', 'Q1803e', 'E03-05', 'E12-09', 'E06-16', 'V2227e', 'E24-03', 'E19-03', 'E14-05', 'Q1802e', 'B0104e', 'E11-04', 'E06-12', 'E09-05', 'E26-28', 'E10-07', 'E15-01', 'E09-09', 'E01-05', 'E15-04', 'B0101e', 'E28-23', 'E27-04', 'E04-03', 'E02-06', 'H0801e', 'E01-04', 'B0102e', 'V2203e', 'U2002e', 'E10-01', 'E10-11', 'E17-04', 'W2341e', 'E12-02', 'W2402e', 'Q1808e', 'E0401e', 'U2072e', 'E06-02', 'E0411e', 'E19-16', 'M1307e', 'E02-10', 'H0803e', 'T1907e', 'E14-03', 'Z2610e', 'E06-06', 'P1636e', 'H0802e', 'E09-03', 'E12-03', 'E10-05', 'E14-01', 'E29-07', 'C0210e', 'E12-01', 'E24-01', 'E29-11', 'E21-03', 'E18-12', 'E25-12', 'E27-05', 'E19-01', 'Z2502e', 'C0214e', 'E06-04', 'E18-13', 'T1920e', 'E13-02', 'E04-10', 'V2102e', 'E06-34', 'E12-07', 'T1908e', 'E06-14', 'O1542e', 'E07-01', 'E23-03', 'E03-03', 'E05-03', 'E02-12', 'E08-08', 'T1917e', 'E06-08', 'E17-05', 'E09-08', 'E10-06', 'U2001e', 'E18-22', 'Z2613e', 'W2450e', 'D0306e', 'E11-03', 'E07-03', 'E04-05'}\n",
      "inference data groups: {'U2074e', 'W2338e', 'T1909e', 'E07-11', 'E23-14', 'U2023e', 'F0540e', 'V2216e', 'O1515e', 'Q1719e', 'F0503e', 'J1036e', 'U2071e', 'E05-07', 'E25-19', 'U2031e', 'X2523e', 'E23-26', 'E23-31', 'U2045e', 'E28-14', 'E08-25', 'V2237e', 'E22-18', 'E26-14', 'E10-18', 'E11-21', 'E20-32', 'V2223e', 'O1514e', 'J1018e', 'P1605e', 'W2301e', 'W2306e', 'E03-09', 'E20-22', 'E28-13', 'Z2511e', 'E14-09', 'U2010e', 'E21-20', 'U2041e', 'I0904e', 'E17-08', 'O1507e', 'E26-42', 'E03-10', 'E28-07', 'R0611e', 'F0518e', 'U2038e', 'E17-09', 'W2425e', 'V2240e', 'E09-16', 'J1034e', 'J1013e', 'P1613e', 'E16-36', 'C0224e', 'E27-27', 'Z2525e', 'E0417e', 'U2037e', 'W2435e', 'P1635e', 'U2042e', 'U2033e', 'R0603e', 'E07-12', 'V2214e', 'E24-21', 'U2040e', 'Q1714e', 'X2507e', 'E27-12', 'W2323e', 'E28-16', 'E09-15', 'E27-13', 'E18-18', 'E16-08', 'T1901e', 'E06-33', 'V2211e', 'L1225e', 'V2228e', 'Q1717e', 'E09-11', 'E07-10', 'L1217e', 'V2132e', 'E24-32', 'J1019e', 'J1022e', 'V2219e', 'E24-27', 'E06-30', 'E13-09', 'E08-24', 'E23-04', 'Q1816e', 'L1205e', 'U2067e', 'W2310e', 'F0531e', 'W2434e', 'U2058e', 'Q1820e', 'E14-22', 'E16-22', 'E22-26', 'E25-11', 'E08-12', 'E26-33', 'E16-11', 'E24-31', 'E26-05', 'O1534e', 'E17-10', 'J1004e', 'L1201e', 'E26-20', 'U2051e', 'E03-15', 'L1231e', 'M1309e', 'X2515e', 'E23-10', 'E16-25', 'X2504e', 'M1323e', 'E24-13', 'E15-10', 'P1626e', 'J1003e', 'U2064e', 'C0216e', 'E20-11', 'L1228e', 'U2009e', 'E24-28', 'W2419e', 'C0220e', 'E26-44', 'P1627e', 'F0515e', 'E24-25-0', 'Z2528e', 'V2121e', 'L1232e', 'F0537e', 'F0535e', 'F0523e', 'H0805e', 'O1537e', 'E03-17', 'E08-32', 'Q1709e', 'E28-12', 'F0542e', 'J1017e', 'W2339e', 'G0715e', 'E14-12', 'U2019e', 'E20-14', 'B0109e', 'E23-15', 'U2065e', 'E06-21', 'N1416e', 'N1409e', 'R0615e', 'Z2516e', 'E26-11', 'E13-07', 'E23-28', 'B0114e', 'E07-09', 'J1002e', 'C0231e', 'U2014e', 'E21-11', 'E14-21', 'R0617e', 'V2208e', 'F0507e', 'O1511e', 'E25-10', 'P1603e', 'E13-12', 'N1408e', 'P1632e', 'W2324e', 'E20-18', 'W2320e', 'E20-30', 'E21-15', 'E27-09', 'U2035e', 'Q1710e', 'E20-10', 'V2230e', 'V2136e', 'E15-06', 'T1916e', 'H0814e', 'L1203e', 'E26-18', 'E0416e', 'Q1812e', 'E14-07', 'E14-20', 'U2034e', 'E20-29', 'E16-26', 'E22-07', 'V2124e', 'E06-19', 'E22-16', 'V2134e', 'E18-08', 'O1522e', 'V2210e', 'E17-07', 'V2105e', 'W2316e', 'Z2519e', 'E16-06', 'V2242e', 'F0538e', 'U2007e', 'X2529e', 'K1102e', 'N1403e', 'E08-31', 'R0608e', 'C0228e', 'E0409e', 'V2206e', 'E26-35', 'E19-15', 'E26-39', 'E24-23', 'U2048e', 'E05-15', 'R0607e', 'E16-20', 'X2519e', 'X2524e', 'E25-17', 'L1224e', 'E26-10', 'W2453e', 'V2241e', 'O1528e', 'E06-20', 'W2328e', 'L1206e', 'E15-15', 'U2069e', 'E12-13', 'L1214e', 'U2063e', 'O1510e', 'E24-14', 'E16-10', 'E16-35', 'E23-30', 'O1505e', 'E20-07', 'E28-15', 'F0543e', 'E16-03', 'E22-20', 'V2212e', 'Q1821e', 'M1306e', 'O1529e', 'M1312e', 'I0910e', 'O1536e', 'V2113e', 'E11-22', 'Q1805e', 'V2127e', 'V2213e', 'X2505e', 'E08-18', 'Z2602e', 'E04-11', 'X2508e', 'I0906e', 'W2309e', 'E19-05', 'O1525e', 'M1310e', 'E28-06', 'K1111e', 'E14-18', 'E27-15', 'V2109e', 'H0812e', 'E21-08', 'E08-21', 'O1540e', 'W2336e', 'E16-37', 'L1207e', 'V2217e', 'X2514e', 'V2116e', 'E14-10', 'O1520e', 'E10-15', 'E23-17', 'P1623e', 'W2407e', 'E08-11', 'E18-05', 'E20-12', 'E17-13', 'C0204e', 'E23-27', 'V2225e', 'O1523e', 'E22-05', 'E14-08', 'E26-43', 'B0107e', 'W2325e', 'L1204e', 'E08-35', 'B0110e', 'E17-06', 'F0526e', 'E26-48', 'K1107e', 'O1504e', 'W2431e', 'E24-19', 'E14-14', 'W2416e', 'P1615e', 'R0609e', 'E20-23', 'R0620e', 'U2066e', 'G0711e', 'G0708e', 'E23-16', 'T1915e', 'E19-10', 'L1236e', 'E21-13', 'W2331e', 'E20-31', 'E16-21', 'L1208e', 'E08-29', 'W2440e', 'E21-10', 'E16-38', 'J1035e', 'O1535e', 'E23-06', 'E16-32', 'E25-08', 'W2318e', 'I0907e', 'E21-21', 'Z2518e', 'X2520e', 'W2329e', 'E15-07', 'E05-12', 'X2527e', 'E22-13', 'N1421e', 'C0222e', 'E06-27', 'E25-07', 'I0913e', 'E0414e', 'E08-10', 'E23-07', 'Q1806e', 'C0203e', 'P1618e', 'E04-13', 'J1033e', 'F0502e', 'E10-16', 'E22-04', 'E25-18', 'E06-24', 'M1322e', 'E14-15', 'E26-21', 'W2304e', 'L1210e', 'Z2517e', 'Z2601e', 'P1641e', 'K1105e', 'E08-27', 'Q1815e', 'E19-09', 'W2445e', 'E27-11', 'W2442e', 'C0232e', 'V2222e', 'W2443e', 'Z2504e', 'W2412e', 'E18-10', 'E13-13', 'P1610e', 'Z2524e', 'E08-33', 'V2243e', 'D0311e', 'F0512e', 'R0604e', 'J1031e', 'I0914e', 'X2525e', 'P1612e', 'U2053e', 'E06-31', 'P1620e', 'E24-08', 'X2517e', 'U2052e', 'E03-04', 'E26-40', 'E26-31', 'T1910e', 'W2327e', 'E24-25-1', 'Q1705e', 'W2312e', 'E0404e', 'U2021e', 'E03-18', 'E26-45', 'E17-12', 'E27-21', 'P1625e', 'O1512e', 'I0912e', 'E10-14', 'E08-28', 'E12-10', 'X2531e', 'Z2526e', 'O1509e', 'E27-25', 'U2005e', 'E06-28', 'E22-19', 'B0106e', 'V2226e', 'U2061e', 'U2075e', 'E21-05', 'G0714e', 'E21-07', 'Q1811e', 'N1413e', 'W2424e', 'E28-11', 'W2333e', 'E19-06', 'W2444e', 'N1419e', 'L1239e', 'E19-13', 'J1023e', 'E24-07', 'V2114e', 'G0710e', 'E27-26', 'W2406e', 'Q1817e', 'Q1807e', 'N1422e', 'W2417e', 'E16-16', 'E24-16', 'U2018e', 'V2224e', 'T1902e', 'E03-11', 'E05-13', 'W2410e', 'Q1706e', 'E06-25', 'W2319e', 'W2421e', 'P1614e', 'W2422e', 'X2516e', 'P1630e', 'E07-13', 'E09-13', 'G0704e', 'W2447e', 'V2238e', 'E21-06', 'V2138e', 'Q1810e', 'U2020e', 'I0903e', 'E03-16', 'J1027e', 'K1112e', 'K1113e', 'Q1824e', 'O1533e', 'E0408e', 'P1617e', 'F0505e', 'E0402e', 'L1222e', 'P1608e', 'Q1809e', 'V2131e', 'M1321e', 'X2506e', 'E03-12', 'E26-12', 'X2512e', 'E24-18', 'E20-27', 'E11-14', 'E20-25', 'Q1703e', 'L1233e', 'E20-09', 'K1101e', 'E27-23', 'U2047e', 'L1219e', 'R0613e', 'E16-30', 'L1213e', 'Z2521e', 'D0316e', 'P1611e', 'W2420e', 'E21-19', 'G0709e', 'J1006e', 'M1314e', 'H0809e', 'E26-22', 'Z2509e', 'W2308e', 'Q1814e', 'E16-23', 'E12-17', 'V2218e', 'E26-49', 'F0517e', 'E20-17', 'I0911e', 'C0223e', 'N1417e', 'U2025e', 'W2432e', 'E22-09', 'P1619e', 'E03-19', 'L1209e', 'V2231e', 'F0533e', 'E15-09', 'E26-32', 'E03-21', 'M1326e', 'O1517e', 'X2518e', 'E16-29', 'V2120e', 'O1513e', 'Z2603e', 'E27-19', 'W2433e', 'E05-14', 'I0905e', 'E26-16', 'E22-15', 'G0705e', 'M1315e', 'P1622e', 'E24-05', 'E21-14', 'E06-29', 'E10-19', 'Z2515e', 'F0544e', 'T1919e', 'P1640e', 'L1226e', 'P1634e', 'E16-14', 'V2233e', 'W2426e', 'E06-18', 'E03-08', 'E22-24', 'F0508e', 'E20-24', 'E24-11', 'R0616e', 'C0207e', 'N1412e', 'G0702e', 'J1030e', 'E21-17', 'Q1702e', 'V2232e', 'F0511e', 'F0527e', 'E28-10', 'B0111e', 'V2221e', 'W2448e', 'Q1708e', 'Z2522e', 'P1629e', 'E0405e', 'E18-20', 'E03-22', 'L1229e', 'I0908e', 'L1216e', 'E23-13', 'D0312e', 'Z2513e', 'G0703e', 'P1624e', 'H0813e', 'V2207e', 'R0619e', 'R0612e', 'P1631e', 'E22-06', 'E23-11', 'E21-23', 'U2013e', 'E13-14', 'U2046e', 'L1238e', 'M1305e', 'P1628e', 'L1240e', 'E18-09', 'O1521e', 'O1532e', 'F0536e', 'U2032e', 'O1526e', 'O1530e', 'E0406e', 'Q1711e', 'V2107e', 'C0227e', 'J1005e', 'E08-16', 'E14-19', 'E26-19', 'E10-13', 'E27-20', 'F0516e', 'W2302e', 'E16-09', 'J1007e', 'E12-16', 'E16-18', 'U2057e', 'N1420e', 'Q1721e', 'E06-22', 'L1218e', 'F0513e', 'E12-12', 'E24-22', 'U2028e', 'O1527e', 'W2303e', 'E24-26', 'E16-27', 'W2429e', 'E27-17', 'F0534e', 'E08-14', 'Z2527e', 'E05-10', 'E20-13', 'Q1822e', 'U2044e', 'L1202e', 'E08-15', 'E04-14', 'G0706e', 'U2029e', 'E11-19', 'E23-33', 'G0707e', 'E26-08', 'P1633e', 'E25-09', 'E28-09', 'W2322e', 'E28-04', 'E0415e', 'L1241e', 'U2026e', 'V2215e', 'E20-06', 'M1316e', 'W2330e', 'M1317e', 'E20-33', 'E16-34', 'L1235e', 'W2452e', 'E22-17', 'V2112e', 'R0614e', 'U2070e', 'V2118e', 'E23-24', 'D0309e', 'E24-20', 'C0205e', 'E23-08', 'E25-05', 'W2317e', 'W2446e', 'E16-24', 'V2106e', 'J1001e', 'I0916e', 'X2509e', 'U2039e', 'E05-09', 'X2522e', 'E14-17', 'M1302e', 'E14-23', 'L1234e', 'E12-14', 'E23-32', 'U2068e', 'U2006e', 'W2413e', 'E0413e', 'U2024e', 'E24-30', 'E20-08', 'N1414e', 'V2244e', 'E18-21', 'E16-19', 'G0713e', 'E03-07', 'J1010e', 'E16-17', 'M1313e', 'O1519e', 'J1032e', 'T1913e', 'Z2501e', 'F0520e', 'E28-08', 'E11-20', 'Q1713e', 'F0509e', 'E20-28', 'E08-30', 'L1212e', 'Q1819e', 'F0541e', 'E17-14', 'E10-17', 'W2418e', 'C0218e', 'P1616e', 'D0310e', 'J1025e', 'J1012e', 'N1402e', 'Q1720e', 'Q1823e', 'W2408e', 'D0307e', 'N1415e', 'D0303e', 'Z2514e', 'V2220e', 'E27-22', 'E22-08', 'E22-25', 'E13-11', 'X2510e', 'E08-22', 'E04-12', 'E13-08', 'K1103e', 'E22-21', 'U2043e', 'I0902e', 'I0909e', 'J1024e', 'U2017e', 'C0229e', 'R0610e', 'V2229e', 'Z2510e', 'K1110e', 'E26-07', 'W2340e', 'E26-09', 'E16-33', 'U2059e', 'C0221e', 'F0530e', 'E23-25', 'B0103e', 'W2430e', 'E19-07', 'D0315e', 'U2015e', 'B0105e', 'E0403e', 'V2108e', 'M1319e', 'E15-14', 'E25-16', 'O1506e', 'P1642e', 'R0605e', 'J1011e', 'W2454e', 'Q1818e', 'E22-23', 'Q1715e', 'U2056e', 'N1404e', 'E18-07', 'C0219e', 'V2119e', 'F0522e', 'E09-12', 'E18-06', 'G0712e', 'K1108e', 'E16-13', 'W2441e', 'E12-15', 'E15-08', 'E16-04', 'U2012e', 'U2008e', 'W2314e', 'J1008e', 'E20-04', 'U2030e', 'E26-23', 'E21-09', 'E24-10', 'E24-15', 'C0211e', 'J1015e', 'E20-05', 'M1303e', 'W2315e', 'P1604e', 'U2011e', 'U2036e', 'W2436e', 'E23-35', 'E03-20', 'E26-17', 'E27-24', 'Q1804e', 'E23-05', 'L1230e', 'V2209e', 'J1026e', 'L1227e', 'X2530e', 'E16-28', 'H0810e', 'E20-15', 'E21-16', 'O1531e', 'E18-19', 'W2404e', 'Q1707e', 'E16-05', 'E22-22', 'O1508e', 'E17-11', 'I0915e', 'E27-18', 'F0504e', 'K1104e', 'W2307e', 'E23-22', 'E14-24', 'U2027e', 'D0308e', 'W2439e', 'E27-10', 'H0811e', 'N1411e', 'C0225e', 'H0804e', 'O1539e', 'V2115e', 'I0901e', 'J1009e', 'B0112e', 'W2437e', 'E14-25', 'E24-17', 'Z2512e', 'C0217e', 'N1410e', 'C0202e', 'B0113e', 'P1621e', 'E24-09', 'X2528e', 'Q1718e', 'O1518e', 'W2411e', 'E23-12', 'H0808e', 'E12-11', 'E05-11', 'V2137e', 'R0606e', 'E23-20', 'W2313e', 'E08-34', 'V2234e', 'G0701e', 'V2133e', 'E13-10', 'E24-12', 'E26-38', 'E11-13', 'E19-14', 'F0528e', 'V2239e', 'X2526e', 'W2415e', 'F0501e', 'K1109e', 'C0206e', 'L1220e', 'W2423e', 'O1516e', 'P1607e', 'E26-06', 'Q1716e', 'W2335e', 'N1418e', 'E06-23', 'E20-26', 'N1406e', 'E23-29', 'T1914e', 'U2060e', 'Z2508e', 'F0506e', 'E21-22', 'E08-17', 'E26-15', 'W2311e', 'E24-06', 'E0407e', 'E28-05', 'E27-07', 'U2022e', 'L1215e', 'V2117e', 'L1223e', 'E26-37', 'Z2520e', 'E27-28', 'P1609e', 'E25-06', 'E27-14', 'T1911e', 'V2110e', 'E23-09', 'E08-20', 'J1016e', 'E23-21', 'E26-30', 'W2449e', 'E14-11', 'E14-13', 'E22-11', 'E23-34', 'E11-12', 'E16-07', 'E08-26', 'V2235e', 'W2332e', 'U2055e', 'U2050e', 'F0529e', 'E23-19', 'E03-13', 'J1028e', 'W2414e', 'L1237e', 'Q1704e', 'P1638e', 'Q1813e', 'W2409e', 'V2122e', 'F0510e', 'M1304e', 'W2427e', 'U2016e', 'M1311e', 'E08-19', 'V2111e', 'M1320e', 'E09-14', 'Q1712e', 'E26-41', 'W2428e', 'D0313e', 'W2326e', 'W2334e', 'E22-12', 'E16-31', 'P1606e', 'M1325e', 'F0525e', 'E08-23', 'J1037e', 'J1029e', 'E16-12', 'P1639e', 'V2236e', 'W2455e', 'E27-16', 'N1405e', 'E06-17', 'Z2523e', 'O1524e', 'M1308e', 'E14-16', 'C0226e', 'C0230e', 'M1301e', 'F0539e', 'O1538e', 'X2513e', 'U2049e', 'W2405e', 'U2054e', 'L1211e', 'E08-13', 'U2062e', 'K1106e', 'M1318e', 'E20-16', 'E26-36', 'X2521e', 'E23-23', 'E06-26', 'N1423e', 'W2321e', 'E03-23', 'J1014e', 'E23-18', 'M1324e', 'D0314e', 'Q1722e', 'E21-18', 'E24-24', 'J1021e', 'L1221e', 'E26-47', 'E22-14', 'N1407e', 'F0521e', 'J1020e', 'E26-13', 'X2511e', 'E05-08', 'T1912e', 'F0524e', 'E27-06', 'W2438e', 'E16-15', 'E0410e', 'Z2507e', 'E19-08', 'V2125e', 'E22-10', 'E27-08', 'E26-34', 'E20-19', 'E13-15', 'E26-46', 'E18-11', 'K1114e', 'E03-14', 'F0532e', 'E21-12', 'V2135e', 'W2305e', 'C0201e', 'L1242e', 'B0108e', 'R0618e', 'W2337e', 'E24-29'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_csv_file(TRAINING_DATA_FILE)\n",
    "training_group = set(df.group)\n",
    "print(f\"training data groups: {set(df.group)}\")\n",
    "\n",
    "df = read_csv_file(INFERENCE_INPUT_DATA_FILE)\n",
    "inference_group = set(df.group)\n",
    "print(f\"inference data groups: {set(df.group)}\")\n",
    "\n",
    "training_group in inference_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing Data Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remapping Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: 362 rows in training data could not be matched.\n",
      "✅ Remapped training data saved to: C:\\github\\table_lense\\pipeline_data\\training_inference\\training\\training_data_remapped.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script: remap_training_data_indices.py\n",
    "Purpose: Remap `original_index` for labeled training data by aligning it with the\n",
    "         source training_inference_data using ['yearbook_source', 'group', 'row_id'].\n",
    "Author: Xiao-Fei Zhang\n",
    "Date: 2025 Mar 24\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from utils.read_csv_file import read_csv_file\n",
    "from project_config import TRAINING_DATA_FILE, TRAINING_INFERENCE_DATA_FILE\n",
    "\n",
    "\n",
    "def remap_original_index(\n",
    "    training_data_file: Path | str,\n",
    "    training_inference_data_file: Path | str,\n",
    "    output_file: Path | str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Replaces the 'original_index' of training data by aligning it with the\n",
    "    original indices from training_inference_data_file based on:\n",
    "    ['yearbook_source', 'group', 'row_id']\n",
    "\n",
    "    Args:\n",
    "        training_data_file: path to the labeled training data (with lost index)\n",
    "        training_inference_data_file: path to the full unlabeled data (with correct index)\n",
    "        output_file: path to save the remapped training data\n",
    "    \"\"\"\n",
    "    # Load both DataFrames\n",
    "    train_df = read_csv_file(training_data_file)\n",
    "    source_df = read_csv_file(training_inference_data_file)\n",
    "\n",
    "    # Ensure key columns exist\n",
    "    key_cols = [\"yearbook_source\", \"group\", \"row_id\"]\n",
    "    for col in key_cols + [\"original_index\"]:\n",
    "        if col not in source_df.columns:\n",
    "            raise ValueError(f\"Missing column '{col}' in source file.\")\n",
    "\n",
    "    for col in key_cols:\n",
    "        if col not in train_df.columns:\n",
    "            raise ValueError(f\"Missing column '{col}' in training file.\")\n",
    "\n",
    "    # Only keep the mapping keys + original_index\n",
    "    source_map = source_df[key_cols + [\"original_index\"]].drop_duplicates()\n",
    "\n",
    "    # Merge to inject the correct original_index into the training data\n",
    "    remapped_df = train_df.drop(columns=[\"original_index\"], errors=\"ignore\").merge(\n",
    "        source_map,\n",
    "        on=key_cols,\n",
    "        how=\"left\",\n",
    "        validate=\"many_to_one\",  # each training row must match at most one row in source\n",
    "    )\n",
    "\n",
    "    missing = remapped_df[\"original_index\"].isna().sum()\n",
    "    if missing > 0:\n",
    "        print(f\"⚠️ Warning: {missing} rows in training data could not be matched.\")\n",
    "\n",
    "    # Save to new file\n",
    "    remapped_df.to_csv(output_file, index=False)\n",
    "    print(f\"✅ Remapped training data saved to: {output_file}\")\n",
    "\n",
    "\n",
    "output_file = r\"C:\\github\\table_lense\\pipeline_data\\training_inference\\training\\training_data_remapped.csv\"\n",
    "remap_original_index(\n",
    "    training_data_file=TRAINING_DATA_FILE,\n",
    "    training_inference_data_file=TRAINING_INFERENCE_DATA_FILE,\n",
    "    output_file=output_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\github\\table_lense\\pipeline_data\\preprocessing\\preprocessed_data_all.csv\n",
      "🔍 Found 0 duplicated key combinations.\n",
      "Empty DataFrame\n",
      "Columns: [yearbook_source, group, row_id, count]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from project_config import PREPROCESSED_ALL_DATA_FILE\n",
    "from utils.read_csv_file import read_csv_file\n",
    "\n",
    "\n",
    "file_path = PREPROCESSED_ALL_DATA_FILE\n",
    "\n",
    "print(file_path)\n",
    "\n",
    "\n",
    "# Load the file\n",
    "df = read_csv_file(file_path)\n",
    "\n",
    "# Group by the merge keys and count occurrences\n",
    "dup_groups = df.groupby([\"yearbook_source\", \"group\", \"row_id\"]).size()\n",
    "\n",
    "# Filter to keep only those with duplicates (count > 1)\n",
    "duplicates = dup_groups[dup_groups > 1].reset_index(name=\"count\")\n",
    "\n",
    "# Show top 10 duplicates\n",
    "print(f\"🔍 Found {len(duplicates)} duplicated key combinations.\")\n",
    "print(duplicates.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deduplicate and reindex training and inference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xzhan\\AppData\\Local\\Temp\\ipykernel_21804\\3367607895.py:39: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cleaned_df = df.groupby(sort_cols, group_keys=False).apply(resolve_duplicates)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned & reindexed file saved to: C:\\github\\table_lense\\pipeline_data\\training_inference\\training_and_inference_data_deduped.csv\n",
      "✅ Final shape: (67882, 9)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script: deduplicate_and_reindex_training_data.py\n",
    "Purpose: Clean up duplicate rows introduced during manual labeling by:\n",
    "    - Sorting consistently\n",
    "    - Keeping labeled rows when conflicts arise\n",
    "    - Rebuilding original_index after cleanup\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from utils.read_csv_file import read_csv_file\n",
    "\n",
    "from project_config import TRAINING_INFERENCE_DATA_FILE\n",
    "\n",
    "\n",
    "def clean_and_reindex_training_data(input_path: Path | str, output_path: Path | str):\n",
    "    # Load file\n",
    "    df = read_csv_file(input_path)\n",
    "\n",
    "    # Define sort order (add more columns if needed)\n",
    "    sort_cols = [\"yearbook_source\", \"group\", \"row_id\"]\n",
    "    df = df.sort_values(by=sort_cols).reset_index(drop=True)\n",
    "\n",
    "    # Mark duplicates based on identifying keys\n",
    "    def resolve_duplicates(group):\n",
    "        if len(group) == 1:\n",
    "            return group\n",
    "\n",
    "        # If one is labeled and the others are \"unlabeled\", keep the labeled one\n",
    "        non_unlabeled = group[group[\"label\"].str.lower() != \"unlabeled\"]\n",
    "        if len(non_unlabeled) == 1:\n",
    "            return non_unlabeled\n",
    "\n",
    "        # Otherwise, just keep the first one\n",
    "        return group.iloc[[0]]\n",
    "\n",
    "    cleaned_df = df.groupby(sort_cols, group_keys=False).apply(resolve_duplicates)\n",
    "\n",
    "    # Rebuild original_index\n",
    "    cleaned_df = cleaned_df.reset_index(drop=True)\n",
    "    cleaned_df[\"original_index\"] = range(len(cleaned_df))\n",
    "\n",
    "    # Save cleaned file\n",
    "    cleaned_df.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Cleaned & reindexed file saved to: {output_path}\")\n",
    "    print(f\"✅ Final shape: {cleaned_df.shape}\")\n",
    "\n",
    "\n",
    "output_file = r\"C:\\github\\table_lense\\pipeline_data\\training_inference\\training_and_inference_data_deduped.csv\"\n",
    "clean_and_reindex_training_data(\n",
    "    input_path=TRAINING_INFERENCE_DATA_FILE,\n",
    "    output_path=output_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 1476.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "folder = Path(r\"C:\\github\\table_lense\\pipeline_data\\output\\reconstructed_tables\")\n",
    "file_count = len([f for f in folder.iterdir() if f.is_file()])\n",
    "print(\"Number of files:\", (file_count - 1) / 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
